{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Learned Representation For Artistic Style\n",
    "steal idea from \n",
    "* https://github.com/pytorch/examples/tree/master/fast_neural_style\n",
    "* https://github.com/kewellcjj/pytorch-multiple-style-transfer\n",
    "* https://github.com/Aftaab99/pytorch-multiple-style-transfer<br>\n",
    "the initialization of instance norm's affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dotmap\n",
    "# !pip install torch\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import utils\n",
    "from transformer_net import TransformerNet\n",
    "from vgg import VGG\n",
    "\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "class default_config:\n",
    "  seed=228922\n",
    "  batch_size=16\n",
    "  big_size=512\n",
    "  image_size=256\n",
    "  style_size=None\n",
    "  dataset=\"val2017\"\n",
    "  lr = 1e-3\n",
    "  epochs = 100\n",
    "  \n",
    "  save_every_n_batches=300\n",
    "  \n",
    "  weight_decay = 0.01\n",
    "  content_weight = 1e5\n",
    "  content_weight_decay = 0.999\n",
    "  content_weight_lower = 1e5\n",
    "  \n",
    "  style_weight = 1e10\n",
    "  \n",
    "  \n",
    "  style_path_template=\"style/*.jpg\"\n",
    "  style_layers = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']\n",
    "  content_layers = [\"relu2_2\"]\n",
    "  \n",
    "  feature_layers = list(set(style_layers+content_layers))\n",
    "  \n",
    "  n_style=len(glob.glob(style_path_template))\n",
    "  \n",
    "config = {k:v for k,v in vars(default_config).items() if k[:2]!= \"__\" and k[-2:]!= \"__\"}\n",
    "args = DotMap(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6878ffa830>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "  transforms.Resize(args.big_size),\n",
    "  transforms.RandomCrop(args.image_size),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Lambda(lambda x: x.mul(255))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(args.dataset, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-551bbd5d645c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_style\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m optimizer = Adam(\n\u001b[1;32m      3\u001b[0m   \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer = TransformerNet(args.n_style).to(device)\n",
    "optimizer = Adam(\n",
    "  transformer.parameters(),\n",
    "  args.lr,\n",
    "  weight_decay=args.weight_decay,\n",
    ")\n",
    "mse_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG(requires_grad=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "imgs_path = glob.glob(args.style_path_template)\n",
    "\n",
    "style_imgs = [\n",
    "  utils.load_image(path, args.style_size)\n",
    "  for path in imgs_path\n",
    "]\n",
    "style_imgs = [style_transform(style) for style in style_imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r,c = ((len(style_imgs)-1)//4+1),4\n",
    "plt.figure(figsize=(r*4,c*4))\n",
    "for idx, (img, path) in enumerate(zip(style_imgs, imgs_path), 1):\n",
    "  plt.subplot(r,c,idx)\n",
    "  plt.title(f\"idx:{idx:03d}, path:{path}\")\n",
    "  plt.imshow(torch.permute(img, (1,2,0)).cpu().numpy().astype(np.uint8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_style = defaultdict(list)\n",
    "for style in style_imgs:\n",
    "  for k,v in vgg(utils.normalize_batch(style.unsqueeze(0).to(device)), args.style_layers).items():\n",
    "    gram_style[k].append(utils.gram_matrix(v))\n",
    "\n",
    "gram_style = {layer_name : torch.cat(gram) for layer_name, gram in gram_style.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "folder = datetime.now().strftime(\"result_%Y-%m-%d-%H-%M\")\n",
    "\n",
    "from pathlib import Path\n",
    "Path(folder).mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "os.remove(\"result\") if os.path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(os.path.join(folder, \"config.json\"), \"w\") as f:\n",
    "  json.dump(config,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doge_x = transforms.ToTensor()(\n",
    "  utils.load_image(\n",
    "    \"doge.jpg\",\n",
    "#     \"amber.jpg\",\n",
    "#     size=args.image_size\n",
    "  )\n",
    ")\n",
    "doge_x = 255*doge_x.repeat(args.n_style, 1, 1, 1)\n",
    "doge_x = doge_x.to(device)\n",
    "\n",
    "test_style_idx = torch.arange(0,args.n_style,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches_pass = 0\n",
    "for epoch in range(args.epochs):\n",
    "  agg_content_loss = 0\n",
    "  agg_style_loss = 0\n",
    "  count = 0\n",
    "  \n",
    "  for batch_id, (x, _) in enumerate(train_loader):\n",
    "    \n",
    "    transformer.train()\n",
    "    n_batch = len(x)\n",
    "    count += n_batch\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    total_batches_pass+=1\n",
    "    \n",
    "    x = x.to(device)\n",
    "    rand_idx = torch.randint(args.n_style, size=(n_batch,), device=device)\n",
    "    y = transformer(x, rand_idx)\n",
    "    \n",
    "    y = utils.normalize_batch(y)\n",
    "    x = utils.normalize_batch(x)\n",
    "    \n",
    "    features_y = vgg(y, args.feature_layers)\n",
    "    features_x = vgg(x, args.content_layers) # do not compute style of this part\n",
    "    \n",
    "    content_loss = 0\n",
    "    for layer_name in args.content_layers:\n",
    "      content_loss += mse_loss(features_y[layer_name], features_x[layer_name])\n",
    "    content_loss *= args.content_weight\n",
    "    args.content_weight *= args.content_weight_decay\n",
    "    args.content_weight = max(args.content_weight, args.content_weight_lower)\n",
    "    \n",
    "    style_loss = 0\n",
    "    for layer_name, gram_source in gram_style.items():\n",
    "      gm_y = utils.gram_matrix(features_y[layer_name])\n",
    "      style_loss += mse_loss(gm_y, torch.index_select(gram_source, 0, rand_idx))\n",
    "    style_loss *= args.style_weight\n",
    "    \n",
    "    total_loss = content_loss+style_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    agg_content_loss += content_loss.item()\n",
    "    agg_style_loss += style_loss.item()\n",
    "    \n",
    "    print(epoch, batch_id)\n",
    "    if batch_id % 50 == 0:\n",
    "      transformer.eval()\n",
    "      with torch.no_grad():\n",
    "        test_result = []\n",
    "        for _test, _idx in zip(doge_x, test_style_idx):\n",
    "          test_result.append(transformer(\n",
    "            _test.unsqueeze(0),\n",
    "            _idx.unsqueeze(0),\n",
    "          ).detach()\n",
    "          )\n",
    "        test_result = torch.cat(test_result)/255\n",
    "        test_result = torch.permute(test_result, (0,2,3,1)).cpu().detach().numpy()\n",
    "        test_result = np.clip(test_result,0,1)\n",
    "\n",
    "      r,c = ((len(style_imgs)-1)//4+1) , 4\n",
    "      plt.figure(figsize=(r*4,c*4))\n",
    "      for idx, img in enumerate(test_result,1):\n",
    "        plt.subplot(r,c,idx)\n",
    "        plt.imshow(img)\n",
    "      \n",
    "      display.clear_output(wait=True)\n",
    "      print(f\"epoch:{epoch}, iter:{batch_id}, current_content_weight:{args.content_weight:.1e}\")\n",
    "      print(f\"agg_content_loss = {agg_content_loss/(batch_id+1)}\\n\"\n",
    "            f\"agg_style_loss = {agg_style_loss/(batch_id+1)}\")\n",
    "      plt.savefig(os.path.join(f\"{folder}\",f\"e{epoch:03d}_b{batch_id:05d}.jpg\"))\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    if total_batches_pass % args.save_every_n_batches == 0:\n",
    "      transformer.eval()\n",
    "      save_model_filename = f\"total-batch{total_batches_pass}_epoch_{epoch}_batch_{batch_id}.pth\"\n",
    "      save_model_path = os.path.join(folder, save_model_filename)\n",
    "      save_model_last_path = os.path.join(folder, \"last.pth\")\n",
    "\n",
    "      torch.save(transformer.state_dict(), save_model_path)\n",
    "      torch.save(transformer.state_dict(), save_model_last_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
